{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition in the Trove Aboriginal Advocate\n",
    "\n",
    "This data source is a XML format dump from the NLA Trove archive of one title - the Aboriginal Advocate.  The data is in the form of a large XML file contianing 3497 articles from this title.  The goal here is to run a named entity recognition process over the documents to extract names of interest. \n",
    "\n",
    "As with other notebooks in this project we will use the SpaCy language processing library to extract names from the text.  The first step is to define a reader for the XML data, this is done in the module [trovereader.py](trovereader.py) which is then imported here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import csv\n",
    "import os\n",
    "import geocoder\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import trovereader\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the source XML filename\n",
    "xmlfile = \"data/nla.obj-573721295_Aborginies_Advocate.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the spacy model we need\n",
    "model = 'en_core_web_lg'\n",
    "#spacy.cli.download(model)\n",
    "nlp = spacy.load(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell uses the trove XML parser to read the separate document records in the XML file and run the NER system over these. The resulting entities are collected into a list of dictionaries which is then converted to a Pandas DataFrame.   We collect all entities that are found and for each one store a bit of context - the entity plus two tokens either side of it.  \n",
    "\n",
    "Since finding entities can take some time, we write out the result to a CSV file.  We first check if the CSV file already exists and if it does, we read the entities from the file rather than recomputing them.  To force the NER process to run, set the variable `force` to `True` at the top of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvname = \"data/Aboriginies_Advocate_Entities.csv\"\n",
    "force = False\n",
    "if not force and os.path.exists(csvname):\n",
    "    entities = pd.read_csv(csvname)\n",
    "else:\n",
    "\n",
    "    entities = []\n",
    "    limit = 10000    # optional limit on how many documents we process\n",
    "\n",
    "    for record in trovereader.trove_parser(xmlfile):\n",
    "        text = record['description'][0]\n",
    "        doc = nlp(text)\n",
    "        for ent in doc.ents:\n",
    "            context = doc[ent.start-2:ent.end+2]\n",
    "            context = \" \".join([w.text for w in context])\n",
    "            d = {'entity': ent.label_, 'label': ent.text, 'context': context, 'doc': record['identifier'][0]}\n",
    "            entities.append(d)\n",
    "        limit -= 1\n",
    "        if limit < 0:\n",
    "            break\n",
    "\n",
    "    entities = pd.DataFrame(entities)\n",
    "    entities.to_csv(csvname, index=False)\n",
    "    \n",
    "entities.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having extracted the entities we can now explore what we have found. Here we look at the locations (GPE) and oganisations (ORG) and see what the most frequent 30 entities are in each case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = entities[entities.entity == \"GPE\"]\n",
    "locations.groupby('label').count().sort_values('entity', ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(locations.shape)\n",
    "print(locations.groupby('label').count().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs = entities[entities.entity == \"ORG\"]\n",
    "orgs.groupby('label').count().sort_values('entity', ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_graph = nx.DiGraph()\n",
    "mention_graph = nx.from_pandas_edgelist(entities, source='doc', target='label', edge_attr=True, create_using=mention_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_graph.number_of_nodes(), mention_graph.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for name, group in locations.groupby('doc'):\n",
    "    locs = group.label\n",
    "    for place1 in group.label:\n",
    "        for place2 in group.label:\n",
    "            if not place1 == place2:\n",
    "                result.append({'loc1': place1, 'loc2': place2, 'doc': name})\n",
    "\n",
    "colloc = pd.DataFrame(result).sort_values(['loc1','loc2'])\n",
    "colloc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collocg = colloc.groupby(['loc1', 'loc2']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'loc1': collocg.index.get_level_values('loc1'), \n",
    "    'loc2': collocg.index.get_level_values('loc2'), \n",
    "    'count': collocg.doc,\n",
    "    'weight': [1/c for c in collocg.doc]   # weight is inverse of count\n",
    "})\n",
    "df.index = range(df.shape[0])  # reset index to integers\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select those collocations that occur more than 10 times\n",
    "df10 = df[df['count'] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = nx.from_pandas_edgelist(df10, source='loc1', target='loc2', edge_attr=True, create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = \"Perkins\"\n",
    "nodes = cg.neighbors(node)\n",
    "nodes = list(nodes)\n",
    "nodes.append(node)\n",
    "subg = cg.subgraph(nodes)\n",
    "print(\"Subgraph of nodes linked to\", node, \"contains\", subg.size(), \"nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "pos = nx.kamada_kawai_layout(subg, weight=\"weight\")\n",
    "colours = [(n==node and'g') or 'y' for n in subg.nodes]\n",
    "ecol = [e[2] for e in subg.edges.data('weight')]\n",
    "ecol = 'grey'\n",
    "nx.draw_networkx(subg, pos, edge_color=ecol, font_weight=\"bold\", font_color='r', arrows=False, node_color=colours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df10[df10.loc1==node].sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- can this data be stored on Github - do we want it to be? \n",
    "- should we look at how to create an Alveo resource from this collection?\n",
    "- full dataset has 3497 records, 100 records yields 6627 entities, so maybe 230k entities all together, what do we do with the resulting entities? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
